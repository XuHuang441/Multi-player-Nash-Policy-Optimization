no change     /n/sw/Mambaforge-23.11.0-0/condabin/conda
no change     /n/sw/Mambaforge-23.11.0-0/bin/conda
no change     /n/sw/Mambaforge-23.11.0-0/bin/conda-env
no change     /n/sw/Mambaforge-23.11.0-0/bin/activate
no change     /n/sw/Mambaforge-23.11.0-0/bin/deactivate
no change     /n/sw/Mambaforge-23.11.0-0/etc/profile.d/conda.sh
no change     /n/sw/Mambaforge-23.11.0-0/etc/fish/conf.d/conda.fish
no change     /n/sw/Mambaforge-23.11.0-0/shell/condabin/Conda.psm1
no change     /n/sw/Mambaforge-23.11.0-0/shell/condabin/conda-hook.ps1
no change     /n/sw/Mambaforge-23.11.0-0/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /n/sw/Mambaforge-23.11.0-0/etc/profile.d/conda.csh
modified      /n/home08/xiaominli/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

Added mamba to /n/home08/xiaominli/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

Starting iteration 1 using model: RLHFlow/LLaMA3-SFT
Starting generation for iteration 1...
model_path RLHFlow/LLaMA3-SFT
Dataset_path RLHFlow/iterative-prompt-v1-iter1-20K
INFO 04-27 20:39:20 __init__.py:207] Automatically detected platform cuda.
INFO 04-27 20:40:15 config.py:549] This model supports multiple tasks: {'score', 'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 04-27 20:40:15 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='RLHFlow/LLaMA3-SFT', speculative_config=None, tokenizer='RLHFlow/LLaMA3-SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=RLHFlow/LLaMA3-SFT, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 04-27 20:40:22 cuda.py:229] Using Flash Attention backend.
INFO 04-27 20:40:23 model_runner.py:1110] Starting to load model RLHFlow/LLaMA3-SFT...
INFO 04-27 20:40:26 weight_utils.py:254] Using model weights format ['*.safetensors']
INFO 04-27 20:40:49 model_runner.py:1115] Loading model weights took 14.9595 GB
INFO 04-27 20:40:53 worker.py:267] Memory profiling takes 3.81 seconds
INFO 04-27 20:40:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB
INFO 04-27 20:40:53 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 54.94GiB.
INFO 04-27 20:40:54 executor_base.py:111] # cuda blocks: 28129, # CPU blocks: 4096
INFO 04-27 20:40:54 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 54.94x
INFO 04-27 20:41:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-27 20:41:28 model_runner.py:1562] Graph capturing finished in 27 secs, took 0.26 GiB
INFO 04-27 20:41:28 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 38.85 seconds
Data Size:5
I collect  2 samples
Saving output to: dataset_path/datasets/tdpo_iter1/data_0.json

model_path RLHFlow/LLaMA3-SFT
Dataset_path RLHFlow/iterative-prompt-v1-iter1-20K
INFO 04-27 20:39:20 __init__.py:207] Automatically detected platform cuda.
INFO 04-27 20:40:15 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
INFO 04-27 20:40:15 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='RLHFlow/LLaMA3-SFT', speculative_config=None, tokenizer='RLHFlow/LLaMA3-SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=RLHFlow/LLaMA3-SFT, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 04-27 20:40:22 cuda.py:229] Using Flash Attention backend.
INFO 04-27 20:40:23 model_runner.py:1110] Starting to load model RLHFlow/LLaMA3-SFT...
INFO 04-27 20:40:26 weight_utils.py:254] Using model weights format ['*.safetensors']
INFO 04-27 20:40:49 model_runner.py:1115] Loading model weights took 14.9595 GB
INFO 04-27 20:40:53 worker.py:267] Memory profiling takes 3.81 seconds
INFO 04-27 20:40:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB
INFO 04-27 20:40:53 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 54.94GiB.
INFO 04-27 20:40:54 executor_base.py:111] # cuda blocks: 28129, # CPU blocks: 4096
INFO 04-27 20:40:54 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 54.94x
INFO 04-27 20:41:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-27 20:41:28 model_runner.py:1562] Graph capturing finished in 27 secs, took 0.26 GiB
INFO 04-27 20:41:28 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 38.91 seconds
Data Size:5
I collect  2 samples
Saving output to: dataset_path/datasets/tdpo_iter1/data_1.json

Merging generation data...
dataset_path/datasets/tdpo_iter1/data_0.json
2
dataset_path/datasets/tdpo_iter1/data_1.json
2
I collect  4 samples
dataset_path/datasets/tdpo_iter1/data.json

Starting preference modeling...
[2025-04-27 20:45:14,790] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

[2025-04-27 20:45:14,791] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Merging preference data...
dataset_path/datasets/tdpo_iter1/pref_0.json

[2025-04-27 20:46:53,513] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:47:40,393] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:47:40,395] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Starting TDPO training for iteration 1...
[2025-04-27 20:48:25,872] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:49:01,848] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:49:01,852] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Merging deepspeed checkpoints...
Completed iteration 1
Starting iteration 2 using model: model_path/models/tdpo_iter1
Starting generation for iteration 2...
model_path model_path/models/tdpo_iter1
Dataset_path RLHFlow/iterative-prompt-v1-iter2-20K
INFO 04-27 20:50:06 __init__.py:207] Automatically detected platform cuda.

model_path model_path/models/tdpo_iter1
Dataset_path RLHFlow/iterative-prompt-v1-iter2-20K
INFO 04-27 20:50:06 __init__.py:207] Automatically detected platform cuda.

Merging generation data...
dataset_path/datasets/tdpo_iter2/data_0.json

Starting preference modeling...
[2025-04-27 20:51:05,897] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

[2025-04-27 20:51:05,896] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Merging preference data...
dataset_path/datasets/tdpo_iter2/pref_0.json

[2025-04-27 20:51:54,355] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:52:27,590] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:52:27,592] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Starting TDPO training for iteration 2...
[2025-04-27 20:53:02,591] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:53:35,215] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:53:35,218] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Merging deepspeed checkpoints...
Completed iteration 2
Starting iteration 3 using model: model_path/models/tdpo_iter2
Starting generation for iteration 3...
model_path model_path/models/tdpo_iter2
Dataset_path RLHFlow/iterative-prompt-v1-iter3-20K
INFO 04-27 20:54:34 __init__.py:207] Automatically detected platform cuda.

model_path model_path/models/tdpo_iter2
Dataset_path RLHFlow/iterative-prompt-v1-iter3-20K
INFO 04-27 20:54:34 __init__.py:207] Automatically detected platform cuda.

Merging generation data...
dataset_path/datasets/tdpo_iter3/data_0.json

Starting preference modeling...
[2025-04-27 20:55:33,211] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

[2025-04-27 20:55:33,214] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Merging preference data...
dataset_path/datasets/tdpo_iter3/pref_0.json

[2025-04-27 20:56:21,407] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:56:54,535] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:56:54,537] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Starting TDPO training for iteration 3...
[2025-04-27 20:57:29,639] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:58:02,238] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 20:58:02,240] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/netscratch/lu_lab/Lab/xiaominli/mycache/triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Merging deepspeed checkpoints...
Completed iteration 3
All iterations completed successfully!
