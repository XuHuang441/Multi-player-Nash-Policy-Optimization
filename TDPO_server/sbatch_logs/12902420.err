[W428 09:01:20.949969129 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [holygpu8a31202.rc.fas.harvard.edu]:42795 (errno: 97 - Address family not supported by protocol).
[W428 09:01:20.950116269 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [holygpu8a31202.rc.fas.harvard.edu]:51327 (errno: 97 - Address family not supported by protocol).
[W428 09:01:20.950148285 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [holygpu8a31202.rc.fas.harvard.edu]:58393 (errno: 97 - Address family not supported by protocol).
[W428 09:01:20.950221175 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [holygpu8a31202.rc.fas.harvard.edu]:56937 (errno: 97 - Address family not supported by protocol).
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.66s/it]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.56s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.91s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.84s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.82s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.87s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.86s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.78s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.29s/it]

./run_tdpo_v2_XM.sh: line 58: 2181047 Killed                  CUDA_VISIBLE_DEVICES=$idx python ./generation/get_hf2.py --model_name_or_path $previous_model --dataset_name_or_path $input_path --output_dir $json_output --sanity_check $sanity_check --K $K --temperature 1.0 --local_index $idx --my_world_size $my_world_size --eos_ids 128009
./run_tdpo_v2_XM.sh: line 58: 2181048 Killed                  CUDA_VISIBLE_DEVICES=$idx python ./generation/get_hf2.py --model_name_or_path $previous_model --dataset_name_or_path $input_path --output_dir $json_output --sanity_check $sanity_check --K $K --temperature 1.0 --local_index $idx --my_world_size $my_world_size --eos_ids 128009
./run_tdpo_v2_XM.sh: line 58: 2181050 Killed                  CUDA_VISIBLE_DEVICES=$idx python ./generation/get_hf2.py --model_name_or_path $previous_model --dataset_name_or_path $input_path --output_dir $json_output --sanity_check $sanity_check --K $K --temperature 1.0 --local_index $idx --my_world_size $my_world_size --eos_ids 128009
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|â–Ž         | 1/35 [00:00<00:14,  2.39it/s]Capturing CUDA graph shapes:   6%|â–Œ         | 2/35 [00:00<00:12,  2.61it/s]Capturing CUDA graph shapes:   9%|â–Š         | 3/35 [00:01<00:11,  2.70it/s]Capturing CUDA graph shapes:  11%|â–ˆâ–        | 4/35 [00:01<00:11,  2.70it/s]Capturing CUDA graph shapes:  14%|â–ˆâ–        | 5/35 [00:01<00:11,  2.68it/s]Capturing CUDA graph shapes:  17%|â–ˆâ–‹        | 6/35 [00:02<00:10,  2.73it/s]Capturing CUDA graph shapes:  20%|â–ˆâ–ˆ        | 7/35 [00:02<00:10,  2.76it/s]Capturing CUDA graph shapes:  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:02<00:09,  2.77it/s]Capturing CUDA graph shapes:  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:03<00:09,  2.79it/s]Capturing CUDA graph shapes:  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:03<00:09,  2.76it/s]Capturing CUDA graph shapes:  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:04<00:08,  2.78it/s]Capturing CUDA graph shapes:  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:04<00:08,  2.80it/s]Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:04<00:07,  2.80it/s]Capturing CUDA graph shapes:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:05<00:07,  2.81it/s]Capturing CUDA graph shapes:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:05<00:07,  2.82it/s]Capturing CUDA graph shapes:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:05<00:06,  2.83it/s]Capturing CUDA graph shapes:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:06<00:06,  2.85it/s]Capturing CUDA graph shapes:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:06<00:05,  2.87it/s]Capturing CUDA graph shapes:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:06<00:05,  2.88it/s]Capturing CUDA graph shapes:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:07<00:05,  2.90it/s]Capturing CUDA graph shapes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:07<00:04,  2.91it/s]Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:07<00:04,  2.83it/s]Capturing CUDA graph shapes:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:08<00:04,  2.86it/s]Capturing CUDA graph shapes:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:08<00:03,  2.88it/s]Capturing CUDA graph shapes:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:08<00:03,  2.87it/s]Capturing CUDA graph shapes:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:09<00:03,  2.89it/s]Capturing CUDA graph shapes:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:09<00:02,  2.91it/s]Capturing CUDA graph shapes:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:09<00:02,  2.94it/s]Capturing CUDA graph shapes:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:10<00:02,  2.95it/s]Capturing CUDA graph shapes:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:10<00:01,  2.97it/s]Capturing CUDA graph shapes:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:10<00:01,  2.98it/s]Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:11<00:01,  2.99it/s]Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:11<00:00,  3.00it/s]Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:11<00:00,  3.01it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:12<00:00,  3.00it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:12<00:00,  2.86it/s]
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:04,  1.49it/s, est. speed input: 124.81 toks/s, output: 482.87 toks/s]Processed prompts:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:04,  1.49it/s, est. speed input: 124.81 toks/s, output: 482.87 toks/s]
[rank0]:[W428 09:02:13.517909334 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 118.13 examples/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 19.63it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 18.90it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 24.66it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 18.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 25.25it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 24.57it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 23.94it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 29.45it/s]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.78s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.78s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.82s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.82s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.95s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.23s/it]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 126.82 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 179.93 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 178.28 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 144.41 examples/s]
[W428 09:04:50.543060539 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 49.20it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 48.30it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 48.34it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 69.31it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 4 examples [00:00, 420.02 examples/s]
[W428 09:05:28.389802001 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W428 09:05:28.519401885 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/tdpo/trainer.py:513: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `PreComputer.__init__`. Use `processing_class` instead.
  super().__init__(
Extracting prompt from train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]Extracting prompt from train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1707.08 examples/s]
/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/tdpo/trainer.py:513: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `PreComputer.__init__`. Use `processing_class` instead.
  super().__init__(
[rank1]:[W428 09:05:29.698446662 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Applying chat template to train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]Applying chat template to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1759.17 examples/s]
Tokenizing train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 409.33 examples/s]
[rank0]:[W428 09:05:29.098507564 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt from train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]Extracting prompt from train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1641.45 examples/s]
Applying chat template to train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]Applying chat template to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1632.50 examples/s]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Tokenizing train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 361.52 examples/s]
Train dataset reference log probs:   0%|          | 0/2 [00:00<?, ?it/s]Train dataset reference log probs:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.23it/s]Train dataset reference log probs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.23it/s]
Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 578.37 examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 568.05 examples/s]
Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 506.99 examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 500.01 examples/s]
[rank0]:[W428 09:05:39.838843690 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[W428 09:06:11.634308496 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 47.83it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 48.49it/s]
[W428 09:06:47.769219472 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W428 09:06:47.995508546 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.
Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: xiaominli (LLM-researchers) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.10
wandb: Run data is saved locally in /n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/wandb/run-20250428_090706-5co83j6w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run model_path/models/tdpo_iter1
wandb: â­ï¸ View project at https://wandb.ai/LLM-researchers/huggingface
wandb: ðŸš€ View run at https://wandb.ai/LLM-researchers/huggingface/runs/5co83j6w
  0%|          | 0/2 [00:00<?, ?it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
                                       0%|          | 0/2 [00:02<?, ?it/s]Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 69, in <module>
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 61, in main
    trainer.train()
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/trainer.py", line 3782, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/accelerator.py", line 2446, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 266, in backward
    self.engine.backward(loss, **kwargs)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2216, in backward
    self._do_optimizer_backward(loss, retain_graph)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2162, in _do_optimizer_backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py", line 2280, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 41.50 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.44 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 69, in <module>
[rank0]:     main()
[rank0]:   File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 61, in main
[rank0]:     trainer.train()
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/trainer.py", line 3782, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/accelerator.py", line 2446, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2216, in backward
[rank0]:     self._do_optimizer_backward(loss, retain_graph)
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2162, in _do_optimizer_backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py", line 2280, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/_tensor.py", line 626, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 41.50 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.44 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0428 09:07:17.614000 2183147 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2183200 closing signal SIGTERM
E0428 09:07:17.897000 2183147 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 2183199) of binary: /n/home08/xiaominli/.conda/envs/NPO/bin/python3.11
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tdpo/tdpo_train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-28_09:07:17
  host      : holygpu8a31202.rc.fas.harvard.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2183199)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
python: can't open file '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/model_path/models/tdpo_iter1/zero_to_fp32.py': [Errno 2] No such file or directory
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/get_hf2.py", line 86, in <module>
    llm = LLM(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
    return fn(*args, **kwargs)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/get_hf2.py", line 86, in <module>
    self.llm_engine = self.engine_class.from_engine_args(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
    llm = LLM(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
    engine_config = engine_args.create_engine_config(usage_context)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/get_hf2.py", line 86, in <module>
    return fn(*args, **kwargs)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    llm = LLM(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
    model_config = self.create_model_config()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    self.llm_engine = self.engine_class.from_engine_args(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
    return fn(*args, **kwargs)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    return ModelConfig(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/config.py", line 304, in __init__
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/get_hf2.py", line 86, in <module>
    engine_config = engine_args.create_engine_config(usage_context)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
    self.llm_engine = self.engine_class.from_engine_args(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
    hf_config = get_config(self.model, trust_remote_code, revision,
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_config
    llm = LLM(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
    model_config = self.create_model_config()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    raise ValueError(f"No supported config format found in {model}.")
ValueError: No supported config format found in model_path/models/tdpo_iter1.
    engine_config = engine_args.create_engine_config(usage_context)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
    return fn(*args, **kwargs)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    return ModelConfig(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/config.py", line 304, in __init__
    model_config = self.create_model_config()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    self.llm_engine = self.engine_class.from_engine_args(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
    hf_config = get_config(self.model, trust_remote_code, revision,
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_config
    return ModelConfig(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/config.py", line 304, in __init__
    engine_config = engine_args.create_engine_config(usage_context)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
    raise ValueError(f"No supported config format found in {model}.")
ValueError: No supported config format found in model_path/models/tdpo_iter1.
    hf_config = get_config(self.model, trust_remote_code, revision,
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_config
    model_config = self.create_model_config()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    raise ValueError(f"No supported config format found in {model}.")
ValueError: No supported config format found in model_path/models/tdpo_iter1.
    return ModelConfig(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/config.py", line 304, in __init__
    hf_config = get_config(self.model, trust_remote_code, revision,
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_config
    raise ValueError(f"No supported config format found in {model}.")
ValueError: No supported config format found in model_path/models/tdpo_iter1.
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/merge_data.py", line 42, in <module>
    ds = load_dataset("json", data_files=my_dir, split="train", field="instances")
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 1849, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 1564, in dataset_module_factory
    ).get_module()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 944, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter2/data_0.json'
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 19.26it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 19.33it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 19.56it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 24.55it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 24.83it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 24.85it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 24.15it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 29.73it/s]
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/annotate_data/get_pref_single.py", line 124, in <module>
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/annotate_data/get_pref_single.py", line 124, in <module>
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/annotate_data/get_pref_single.py", line 124, in <module>
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 913, in get_module
    ).get_module()
      ^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 913, in get_module
    ).get_module()
      ^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 913, in get_module
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 690, in from_patterns
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 690, in from_patterns
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 690, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 583, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 583, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 583, in from_patterns
    resolve_pattern(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    resolve_pattern(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    resolve_pattern(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter2/data_0.json'
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter2/data_3.json'
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter2/data_1.json'
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/annotate_data/get_pref_single.py", line 124, in <module>
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 913, in get_module
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 690, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 583, in from_patterns
    resolve_pattern(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter2/data_2.json'
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/n/home08/xiaominli/.conda/envs/NPO/bin/python3.11', 'annotate_data/get_pref_single.py', '--use_tournament', 'True', '--dataset_name_or_path', 'dataset_path/datasets/tdpo_iter2/data_0.json', '--output_dir', 'dataset_path/datasets/tdpo_iter2/pref_0.json', '--K', '8']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/n/home08/xiaominli/.conda/envs/NPO/bin/python3.11', 'annotate_data/get_pref_single.py', '--use_tournament', 'True', '--dataset_name_or_path', 'dataset_path/datasets/tdpo_iter2/data_2.json', '--output_dir', 'dataset_path/datasets/tdpo_iter2/pref_2.json', '--K', '8']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/n/home08/xiaominli/.conda/envs/NPO/bin/python3.11', 'annotate_data/get_pref_single.py', '--use_tournament', 'True', '--dataset_name_or_path', 'dataset_path/datasets/tdpo_iter2/data_3.json', '--output_dir', 'dataset_path/datasets/tdpo_iter2/pref_3.json', '--K', '8']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/n/home08/xiaominli/.conda/envs/NPO/bin/python3.11', 'annotate_data/get_pref_single.py', '--use_tournament', 'True', '--dataset_name_or_path', 'dataset_path/datasets/tdpo_iter2/data_1.json', '--output_dir', 'dataset_path/datasets/tdpo_iter2/pref_1.json', '--K', '8']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./annotate_data/merge.py", line 42, in <module>
    ds = load_dataset("json", data_files=my_dir, split="train", field="instances")
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 1849, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 1564, in dataset_module_factory
    ).get_module()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 944, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter2/pref_0.json'
[W428 09:10:19.259846786 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/precompute.py", line 139, in <module>
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/precompute.py", line 74, in main
    random_model = AutoModelForCausalLM.from_pretrained(args.base_model_path, torch_dtype=torch.bfloat16)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 531, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1151, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in model_path/models/tdpo_iter1. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/precompute.py", line 139, in <module>
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/precompute.py", line 74, in main
    random_model = AutoModelForCausalLM.from_pretrained(args.base_model_path, torch_dtype=torch.bfloat16)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 531, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1151, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in model_path/models/tdpo_iter1. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth
W0428 09:10:56.497000 2183639 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2183874 closing signal SIGTERM
E0428 09:10:56.516000 2183639 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 2183873) of binary: /n/home08/xiaominli/.conda/envs/NPO/bin/python3.11
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tdpo/precompute.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-28_09:10:56
  host      : holygpu8a31202.rc.fas.harvard.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2183873)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[W428 09:11:26.291336103 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 69, in <module>
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 69, in <module>
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 29, in main
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 29, in main
    tokenizer = AutoTokenizer.from_pretrained(args.base_model_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 966, in from_pretrained
    tokenizer = AutoTokenizer.from_pretrained(args.base_model_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 966, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1151, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1151, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in model_path/models/tdpo_iter1. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth
    raise ValueError(
ValueError: Unrecognized model in model_path/models/tdpo_iter1. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth
W0428 09:12:02.462000 2183952 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2184000 closing signal SIGTERM
E0428 09:12:02.516000 2183952 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 2184001) of binary: /n/home08/xiaominli/.conda/envs/NPO/bin/python3.11
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tdpo/tdpo_train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-28_09:12:02
  host      : holygpu8a31202.rc.fas.harvard.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2184001)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
python: can't open file '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/model_path/models/tdpo_iter2/zero_to_fp32.py': [Errno 2] No such file or directory
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/get_hf2.py", line 86, in <module>
    llm = LLM(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
    return fn(*args, **kwargs)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    self.llm_engine = self.engine_class.from_engine_args(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/get_hf2.py", line 86, in <module>
    engine_config = engine_args.create_engine_config(usage_context)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
    llm = LLM(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
    model_config = self.create_model_config()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    return fn(*args, **kwargs)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    return ModelConfig(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/config.py", line 304, in __init__
    self.llm_engine = self.engine_class.from_engine_args(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/get_hf2.py", line 86, in <module>
    hf_config = get_config(self.model, trust_remote_code, revision,
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_config
    engine_config = engine_args.create_engine_config(usage_context)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
    llm = LLM(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/get_hf2.py", line 86, in <module>
    llm = LLM(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
    raise ValueError(f"No supported config format found in {model}.")
ValueError: No supported config format found in model_path/models/tdpo_iter2.
    model_config = self.create_model_config()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    return fn(*args, **kwargs)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    return fn(*args, **kwargs)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    return ModelConfig(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/config.py", line 304, in __init__
    self.llm_engine = self.engine_class.from_engine_args(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
    self.llm_engine = self.engine_class.from_engine_args(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
    hf_config = get_config(self.model, trust_remote_code, revision,
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_config
    engine_config = engine_args.create_engine_config(usage_context)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
    engine_config = engine_args.create_engine_config(usage_context)
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
    raise ValueError(f"No supported config format found in {model}.")
ValueError: No supported config format found in model_path/models/tdpo_iter2.
    model_config = self.create_model_config()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    model_config = self.create_model_config()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    return ModelConfig(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/config.py", line 304, in __init__
    return ModelConfig(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/config.py", line 304, in __init__
    hf_config = get_config(self.model, trust_remote_code, revision,
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_config
    hf_config = get_config(self.model, trust_remote_code, revision,
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_config
    raise ValueError(f"No supported config format found in {model}.")
ValueError: No supported config format found in model_path/models/tdpo_iter2.
    raise ValueError(f"No supported config format found in {model}.")
ValueError: No supported config format found in model_path/models/tdpo_iter2.
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./generation/merge_data.py", line 42, in <module>
    ds = load_dataset("json", data_files=my_dir, split="train", field="instances")
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 1849, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 1564, in dataset_module_factory
    ).get_module()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 944, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter3/data_0.json'
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 19.40it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 18.98it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 24.36it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 17.72it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 24.85it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 26.48it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 23.45it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 27.74it/s]
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/annotate_data/get_pref_single.py", line 124, in <module>
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/annotate_data/get_pref_single.py", line 124, in <module>
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/annotate_data/get_pref_single.py", line 124, in <module>
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 913, in get_module
    ).get_module()
      ^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 913, in get_module
    ).get_module()
      ^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 913, in get_module
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 690, in from_patterns
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 690, in from_patterns
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 690, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 583, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 583, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 583, in from_patterns
    resolve_pattern(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    resolve_pattern(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    resolve_pattern(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter3/data_2.json'
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter3/data_0.json'
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter3/data_3.json'
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/annotate_data/get_pref_single.py", line 124, in <module>
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/load.py", line 913, in get_module
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 690, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 583, in from_patterns
    resolve_pattern(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter3/data_1.json'
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/n/home08/xiaominli/.conda/envs/NPO/bin/python3.11', 'annotate_data/get_pref_single.py', '--use_tournament', 'True', '--dataset_name_or_path', 'dataset_path/datasets/tdpo_iter3/data_2.json', '--output_dir', 'dataset_path/datasets/tdpo_iter3/pref_2.json', '--K', '8']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/n/home08/xiaominli/.conda/envs/NPO/bin/python3.11', 'annotate_data/get_pref_single.py', '--use_tournament', 'True', '--dataset_name_or_path', 'dataset_path/datasets/tdpo_iter3/data_1.json', '--output_dir', 'dataset_path/datasets/tdpo_iter3/pref_1.json', '--K', '8']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/n/home08/xiaominli/.conda/envs/NPO/bin/python3.11', 'annotate_data/get_pref_single.py', '--use_tournament', 'True', '--dataset_name_or_path', 'dataset_path/datasets/tdpo_iter3/data_3.json', '--output_dir', 'dataset_path/datasets/tdpo_iter3/pref_3.json', '--K', '8']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/n/home08/xiaominli/.conda/envs/NPO/bin/python3.11', 'annotate_data/get_pref_single.py', '--use_tournament', 'True', '--dataset_name_or_path', 'dataset_path/datasets/tdpo_iter3/data_0.json', '--output_dir', 'dataset_path/datasets/tdpo_iter3/pref_0.json', '--K', '8']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./annotate_data/merge.py", line 42, in <module>
    ds = load_dataset("json", data_files=my_dir, split="train", field="instances")
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 1849, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 1564, in dataset_module_factory
    ).get_module()
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/load.py", line 944, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/n/home08/xiaominli/.local/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/dataset_path/datasets/tdpo_iter3/pref_0.json'
[W428 09:15:05.698812938 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/precompute.py", line 139, in <module>
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/precompute.py", line 139, in <module>
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/precompute.py", line 74, in main
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/precompute.py", line 74, in main
    random_model = AutoModelForCausalLM.from_pretrained(args.base_model_path, torch_dtype=torch.bfloat16)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 531, in from_pretrained
    random_model = AutoModelForCausalLM.from_pretrained(args.base_model_path, torch_dtype=torch.bfloat16)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 531, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1151, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1151, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in model_path/models/tdpo_iter2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth
    raise ValueError(
ValueError: Unrecognized model in model_path/models/tdpo_iter2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth
W0428 09:15:41.539000 2185049 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2185189 closing signal SIGTERM
E0428 09:15:41.593000 2185049 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 2185188) of binary: /n/home08/xiaominli/.conda/envs/NPO/bin/python3.11
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tdpo/precompute.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-28_09:15:41
  host      : holygpu8a31202.rc.fas.harvard.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2185188)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[W428 09:16:11.551039762 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 69, in <module>
Traceback (most recent call last):
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 69, in <module>
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 29, in main
    main()
  File "/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/./tdpo/tdpo_train.py", line 29, in main
    tokenizer = AutoTokenizer.from_pretrained(args.base_model_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 966, in from_pretrained
    tokenizer = AutoTokenizer.from_pretrained(args.base_model_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 966, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1151, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1151, in from_pretrained
    raise ValueError(
ValueError: Unrecognized model in model_path/models/tdpo_iter2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth
    raise ValueError(
ValueError: Unrecognized model in model_path/models/tdpo_iter2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth
W0428 09:16:47.646000 2185410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2185462 closing signal SIGTERM
E0428 09:16:47.700000 2185410 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 2185463) of binary: /n/home08/xiaominli/.conda/envs/NPO/bin/python3.11
Traceback (most recent call last):
  File "/n/home08/xiaominli/.conda/envs/NPO/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home08/xiaominli/.conda/envs/NPO/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tdpo/tdpo_train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-28_09:16:47
  host      : holygpu8a31202.rc.fas.harvard.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2185463)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
python: can't open file '/n/netscratch/lu_lab/Lab/xiaominli/LLMResearch/Multi-player-Nash-Policy-Optimization/TDPO_server/model_path/models/tdpo_iter3/zero_to_fp32.py': [Errno 2] No such file or directory
slurmstepd: error: Detected 3 oom_kill events in StepId=12902420.batch. Some of the step tasks have been OOM Killed.
